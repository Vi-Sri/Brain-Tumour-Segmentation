* * * * * * * * * *
161686
* * * * * * * * * *
Tue Apr 23 03:36:56 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:5E:00.0 Off |                    0 |
| N/A   22C    P0             25W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           On  |   00000000:86:00.0 Off |                    0 |
| N/A   23C    P0             24W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
[2024-04-23 03:37:00,831] torch.distributed.run: [WARNING] 
[2024-04-23 03:37:00,831] torch.distributed.run: [WARNING] *****************************************
[2024-04-23 03:37:00,831] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-23 03:37:00,831] torch.distributed.run: [WARNING] *****************************************
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav34.so': /usr/lib/libibverbs/libmlx4-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'librxe-rdmav34.so': librxe-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav34.so': /usr/lib/libibverbs/libmlx4-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'librxe-rdmav34.so': librxe-rdmav34.so: cannot open shared object file: No such file or directory
time elapsed before training: 1.681990146636963
----------
epoch 1/50
Traceback (most recent call last):
  File "/home/sriniana/projects/monai_brats/Brain-Tumour-Segmentation/train_monai_1fold_distributed.py", line 189, in <module>
    main()
  File "/home/sriniana/projects/monai_brats/Brain-Tumour-Segmentation/train_monai_1fold_distributed.py", line 186, in main
    main_worker(args=args)
  File "/home/sriniana/projects/monai_brats/Brain-Tumour-Segmentation/train_monai_1fold_distributed.py", line 100, in main_worker
    epoch_loss = train(train_loader, model, loss_function, optimizer, lr_scheduler, scaler)
  File "/home/sriniana/projects/monai_brats/Brain-Tumour-Segmentation/train_monai_1fold_distributed.py", line 138, in train
    loss = criterion(outputs, batch_data["label"])
  File "/home/sriniana/anaconda3/envs/mic2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/sriniana/anaconda3/envs/mic2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/sriniana/anaconda3/envs/mic2/lib/python3.10/site-packages/monai/losses/dice.py", line 936, in forward
    dice_loss = self.dice(input, target)
  File "/home/sriniana/anaconda3/envs/mic2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/sriniana/anaconda3/envs/mic2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/sriniana/anaconda3/envs/mic2/lib/python3.10/site-packages/monai/losses/dice.py", line 177, in forward
    intersection = torch.sum(target * input, dim=reduce_axis)
  File "/home/sriniana/anaconda3/envs/mic2/lib/python3.10/site-packages/monai/data/meta_tensor.py", line 282, in __torch_function__
    ret = super().__torch_function__(func, types, args, kwargs)
  File "/home/sriniana/anaconda3/envs/mic2/lib/python3.10/site-packages/torch/_tensor.py", line 1418, in __torch_function__
    ret = func(*args, **kwargs)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
[2024-04-23 03:37:30,873] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1945599 closing signal SIGTERM
[2024-04-23 03:37:31,137] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 1945600) of binary: /home/sriniana/anaconda3/envs/mic2/bin/python
Traceback (most recent call last):
  File "/home/sriniana/anaconda3/envs/mic2/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/sriniana/anaconda3/envs/mic2/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/home/sriniana/anaconda3/envs/mic2/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/home/sriniana/anaconda3/envs/mic2/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/home/sriniana/anaconda3/envs/mic2/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/sriniana/anaconda3/envs/mic2/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_monai_1fold_distributed.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-04-23_03:37:30
  host      : c3-1.local
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1945600)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
